{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### COMPANIES NER MODEL TRAINING\n",
        "\n"
      ],
      "metadata": {
        "id": "Kjqf6k2hTgcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Excel file\n",
        "companies_df = pd.read_csv(\"/content/companies.csv\")\n",
        "\n",
        "# Extract the list of companies\n",
        "companies = companies_df['Entity'].tolist()"
      ],
      "metadata": {
        "id": "ouoe_DN2Nia8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "\n",
        "# Load a blank model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the NER component to the pipeline if not already present\n",
        "if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add label to the NER\n",
        "ner.add_label(\"COMPANY\")\n",
        "\n",
        "# Prepare training data\n",
        "TRAIN_DATA = []\n",
        "for company in companies:\n",
        "    # Generate a dummy sentence containing the parameter name\n",
        "    text = f\"{company} is a Company.\"\n",
        "    entities = [(text.index(company), text.index(company) + len(company), \"COMPANY\")]\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    TRAIN_DATA.append(example)\n",
        "\n",
        "# Print a sample of the training data\n",
        "print(TRAIN_DATA)"
      ],
      "metadata": {
        "id": "tp70wWmXNrl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nlp.begin_training()\n",
        "for epoch in range(10):\n",
        "    losses = {}\n",
        "    for example in TRAIN_DATA:\n",
        "        nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(f\"Epoch {epoch}: {losses}\")\n"
      ],
      "metadata": {
        "id": "tlVsDmpaOHfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk(\"company_ner_model\")\n"
      ],
      "metadata": {
        "id": "vdV_XgHUOJ2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PARAMETERS NER MODEL TRAINING\n"
      ],
      "metadata": {
        "id": "2YG5TFpoT5O8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr5yAPJ4u8IT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "parameter_df = pd.read_csv(\"/content/parameters.csv\")\n",
        "\n",
        "# Extract the list of companies\n",
        "parameters = parameter_df['Parameter'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "\n",
        "# Load a blank model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the NER component to the pipeline if not already present\n",
        "if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add label to the NER\n",
        "ner.add_label(\"PARAMETER\")\n",
        "\n",
        "# Prepare training data\n",
        "TRAIN_DATA = []\n",
        "for parameter in parameters:\n",
        "    # Generate a dummy sentence containing the parameter name\n",
        "    text = f\"{parameter} is an important metric.\"\n",
        "    entities = [(text.index(parameter), text.index(parameter) + len(parameter), \"PARAMETER\")]\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    TRAIN_DATA.append(example)\n",
        "\n",
        "# Print a sample of the training data\n",
        "print(TRAIN_DATA)"
      ],
      "metadata": {
        "id": "oOE-TJqXw97o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nlp.begin_training()\n",
        "for epoch in range(10):\n",
        "    losses = {}\n",
        "    for example in TRAIN_DATA:\n",
        "        nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(f\"Epoch {epoch}: {losses}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Qu_aOkx0QC",
        "outputId": "fd1af76f-384e-4c69-f7b6-41158c20dbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: {'ner': 394.3893275024216}\n",
            "Epoch 1: {'ner': 20.405189905991286}\n",
            "Epoch 2: {'ner': 12.463982714850822}\n",
            "Epoch 3: {'ner': 23.62208231996258}\n",
            "Epoch 4: {'ner': 10.125772987221822}\n",
            "Epoch 5: {'ner': 0.012997428566311484}\n",
            "Epoch 6: {'ner': 2.0766805450501833}\n",
            "Epoch 7: {'ner': 0.00012774651432994125}\n",
            "Epoch 8: {'ner': 10.991360883314835}\n",
            "Epoch 9: {'ner': 0.05212024174994202}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk(\"parameter_ner_model\")\n"
      ],
      "metadata": {
        "id": "I2wPHi2vzkDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the custom model\n",
        "nlp = spacy.load(\"/content/company_ner_model/\")\n",
        "\n",
        "def extract_companies(text):\n",
        "    doc = nlp(text)\n",
        "    companies = [ent.text for ent in doc.ents if ent.label_ == \"COMPANY\"]\n",
        "    return companies\n",
        "\n",
        "# Example usage\n",
        "text = \"Apple Inc. and Microsoft Corporation are major tech companies.\"\n",
        "companies = extract_companies(text)\n",
        "print(companies)  # Output should include detected company names"
      ],
      "metadata": {
        "id": "5KZFSWRsz3Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cookies = 'ENTER YOUR COOKIES'"
      ],
      "metadata": {
        "id": "woJlBBNb4lX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "MZEfqZMP4AV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-gemini-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0MpIyJGxqut",
        "outputId": "e5626700-49b1-44b4-b328-cf25b09c5636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-gemini-api\n",
            "  Downloading python_gemini_api-2.4.12-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->python-gemini-api)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from python-gemini-api) (2.32.3)\n",
            "Collecting browser-cookie3 (from python-gemini-api)\n",
            "  Downloading browser_cookie3-0.19.1-py3-none-any.whl.metadata (632 bytes)\n",
            "Collecting loguru (from python-gemini-api)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from python-gemini-api) (2.8.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from python-gemini-api) (3.10.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->python-gemini-api)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->python-gemini-api) (4.0.3)\n",
            "Collecting lz4 (from browser-cookie3->python-gemini-api)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting pycryptodomex (from browser-cookie3->python-gemini-api)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: jeepney in /usr/lib/python3/dist-packages (from browser-cookie3->python-gemini-api) (0.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->python-gemini-api) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->python-gemini-api) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->python-gemini-api) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->python-gemini-api) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->python-gemini-api) (2.0.7)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->python-gemini-api)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->python-gemini-api)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->python-gemini-api) (1.2.2)\n",
            "Downloading python_gemini_api-2.4.12-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading browser_cookie3-0.19.1-py3-none-any.whl (14 kB)\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pycryptodomex, lz4, loguru, hyperframe, hpack, h11, httpcore, h2, browser-cookie3, httpx, python-gemini-api\n",
            "Successfully installed browser-cookie3-0.19.1 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.0 hyperframe-6.0.1 loguru-0.7.2 lz4-4.3.3 pycryptodomex-3.20.0 python-gemini-api-2.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1 with Custom NER Model"
      ],
      "metadata": {
        "id": "zxE3wwXDUlA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text, models, entity_type=\"ORG\"):\n",
        "    entities = set()\n",
        "\n",
        "    # Ensure models is a list\n",
        "    if not isinstance(models, list):\n",
        "        models = [models]\n",
        "\n",
        "    for model in models:\n",
        "        doc = model(text)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == entity_type:\n",
        "                entities.add(ent.text)\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "lxtI6oR7UkdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(client, query, history=[]):\n",
        "    # Extract entities directly from the query\n",
        "    company_entities = extract_entities(query, nlp_model_1, entity_type=\"COMPANY\")\n",
        "    parameter_entities = extract_entities(query, nlp_model_2, entity_type=\"PARAMETER\")\n",
        "\n",
        "    # Determine the company name and parameter from the extracted entities\n",
        "    company_name = next(iter(company_entities), None)\n",
        "    parameter = next(iter(parameter_entities), None)\n",
        "\n",
        "    # Use defaults or history for missing information\n",
        "    start_date, end_date = parse_dates(query)\n",
        "    if not company_name and history:\n",
        "        company_name = history[-1].get(\"entity\")\n",
        "    if not parameter and history:\n",
        "        parameter = history[-1].get(\"parameter\")\n",
        "\n",
        "    # Build the result JSON\n",
        "    result = {\n",
        "        \"entity\": company_name,\n",
        "        \"parameter\": parameter,\n",
        "        \"startDate\": start_date,\n",
        "        \"endDate\": end_date\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "8ECsaglZUoFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_queries(queries):\n",
        "    final_output = []\n",
        "    history = []\n",
        "\n",
        "    for query in queries:\n",
        "        result = process_query(client, query, history)\n",
        "        history.append(result)\n",
        "        final_output.append(result)\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Example queries\n",
        "queries = [\n",
        "    \"What is the GMV of Flipkart for 2023?\",\n",
        "    \"Compare our product with Amazon's offerings.\"\n",
        "]\n",
        "nlp_model_1 = spacy.load(\"/content/drive/MyDrive/company_ner_model\")\n",
        "nlp_model_2 = spacy.load(\"/content/drive/MyDrive/parameter_ner_model\")\n",
        "\n",
        "# Call handle_queries with your queries\n",
        "output = handle_queries(queries)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QzE7Kf-UwEC",
        "outputId": "25009024-c6c1-4bf3-dcd9-849d0d341414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'What', 'parameter': 'What', 'startDate': '2023-08-12', 'endDate': '2024-08-11'}, {'entity': \"Amazon's\", 'parameter': \"Compare our product with Amazon's offerings\", 'startDate': '2023-08-12', 'endDate': '2024-08-11'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2 getting Ouptut from LLM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TYZ_70uIVgkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gemini import Gemini\n"
      ],
      "metadata": {
        "id": "qUUbR4Z0Xsz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Gemini(cookies=cookies)\n"
      ],
      "metadata": {
        "id": "olTBHt_aXtsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "6SWAq_5lZfW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_query(user_query):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from the user query:\n",
        "    - Entity: Company name\n",
        "    - Parameter: Performance metric\n",
        "    - Start Date: Start date of the period (default to one year ago if not mentioned)\n",
        "    - End Date: End date of the period (default to today if not mentioned)\n",
        "\n",
        "    User Query: \"{user_query}\"\n",
        "\n",
        "    Your response should be in the following JSON format:\n",
        "    [\n",
        "        {{\n",
        "            \"entity\": \"<company_name>\",\n",
        "            \"parameter\": \"<metric_name>\",\n",
        "            \"startDate\": \"<start_date_iso>\",\n",
        "            \"endDate\": \"<end_date_iso>\"\n",
        "        }}\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate content using Gemini API\n",
        "    response = client.generate_content(prompt)\n",
        "\n",
        "    # Print the raw response to debug\n",
        "    print(\"Raw response payload:\", response.payload)\n",
        "\n",
        "    # Extract the text from the response payload\n",
        "    response_text = response.payload.get('candidates', [{}])[0].get('text', '')\n",
        "\n",
        "    # Strip Markdown code block formatting and parse JSON\n",
        "    json_text = re.sub(r'^```json\\n|\\n```$', '', response_text).strip()\n",
        "\n",
        "    return json_text\n",
        "\n",
        "def process_query(user_query):\n",
        "    # Set default dates\n",
        "    today = datetime.now()\n",
        "    start_date = (today - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
        "    end_date = today.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Generate the response using Gemini API\n",
        "    response_text = generate_query(user_query)\n",
        "\n",
        "    try:\n",
        "        # Ensure the response is valid JSON\n",
        "        result = json.loads(response_text)\n",
        "        # Replace placeholders for default dates\n",
        "        for item in result:\n",
        "            if 'startDate' not in item or not item['startDate']:\n",
        "                item['startDate'] = start_date\n",
        "            if 'endDate' not in item or not item['endDate']:\n",
        "                item['endDate'] = end_date\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"JSON Decode Error:\", response_text)\n",
        "        result = {\"error\": \"Failed to decode JSON from the response\"}\n",
        "    except Exception as e:\n",
        "        print(\"Exception:\", str(e))\n",
        "        result = {\"error\": f\"An error occurred: {str(e)}\"}\n",
        "\n",
        "    return result\n",
        "\n",
        "def handle_queries(queries):\n",
        "    combined_result = []\n",
        "    parameter_from_first_query = None\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        result = process_query(query)\n",
        "        if isinstance(result, list):\n",
        "            for item in result:\n",
        "                if item.get('parameter'):\n",
        "                    parameter_from_first_query = item['parameter']\n",
        "\n",
        "                # Fill in missing parameters with the first parameter if it's available\n",
        "                if item.get('parameter') is None and parameter_from_first_query:\n",
        "                    item['parameter'] = parameter_from_first_query\n",
        "\n",
        "            combined_result.extend(result)\n",
        "        else:\n",
        "            print(f\"Error processing query '{query}': {result}\")\n",
        "\n",
        "    return combined_result\n",
        "\n",
        "# Example usage\n",
        "queries = [\n",
        "    \"Get me Tesla revenue for the last one year\",\n",
        "    \"Amazon profit for last one year\"\n",
        "]\n",
        "\n",
        "print(handle_queries(queries))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ9amDb3aDJ3",
        "outputId": "6ca795fc-7133-43e8-f2ce-1fdef4e52be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response payload: {'metadata': ['c_5baad7c4b0d684a7', 'r_00e275e05d307cc2'], 'prompt_class': None, 'prompt_candidates': [], 'candidates': [{'rcid': 'rc_f3aea45add4cfe47', 'text': '```json\\n[\\n    {\\n        \"entity\": \"Tesla\",\\n        \"parameter\": \"revenue\",\\n        \"startDate\": \"2023-08-11\",\\n        \"endDate\": \"2024-08-11\"\\n    }\\n]\\n```\\n', 'code': {'snippett_01': '```json\\n[\\n    {\\n        \"entity\": \"Tesla\",\\n        \"parameter\": \"revenue\",\\n        \"startDate\": \"2023-08-11\",\\n        \"endDate\": \"2024-08-11\"\\n    }\\n]\\n```'}, 'web_images': [], 'generated_images': []}]}\n",
            "Raw response payload: {'metadata': ['c_5baad7c4b0d684a7', 'r_ea636d2a05140ca3'], 'prompt_class': None, 'prompt_candidates': [], 'candidates': [{'rcid': 'rc_accc98f6ac036743', 'text': '```json\\n[\\n    {\\n        \"entity\": \"Amazon\",\\n        \"parameter\": \"profit\",\\n        \"startDate\": \"2023-08-11\",\\n        \"endDate\": \"2024-08-11\"\\n    }\\n]\\n```\\n', 'code': {'snippett_01': '```json\\n[\\n    {\\n        \"entity\": \"Amazon\",\\n        \"parameter\": \"profit\",\\n        \"startDate\": \"2023-08-11\",\\n        \"endDate\": \"2024-08-11\"\\n    }\\n]\\n```'}, 'web_images': [], 'generated_images': []}]}\n",
            "[{'entity': 'Tesla', 'parameter': 'revenue', 'startDate': '2023-08-11', 'endDate': '2024-08-11'}, {'entity': 'Amazon', 'parameter': 'profit', 'startDate': '2023-08-11', 'endDate': '2024-08-11'}]\n"
          ]
        }
      ]
    }
  ]
}